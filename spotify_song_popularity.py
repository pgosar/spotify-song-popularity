# -*- coding: utf-8 -*-
"""spotify-song-popularity.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MexvSoSiDfk8ltSVPqFCetfTq8p-B_pF

# Predicting the number of streams of a Song

This dataset contains data from various streaming platforms such as Spotify, Apple Music, and Deezer. It also contains data about the song itself, such as the BPM and the dancability of the song. We will use this data to predict the number of streams the song will get.
<br/><br/>

In order to do this, we will:

1. Clean the data
2. Engineer features from other features
3. Reduce the dimensionality of the data
4. Model the data with different regressors
5. Evaluate each model, primairly using the Mean Error
6. Record the results of our testing
"""

!pip install 'pyclustertend==1.4.9'

import pandas as pd
import numpy as np
import chardet
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_absolute_error
import math
import matplotlib.pyplot as plt
from collections import defaultdict
import statistics
from pyclustertend import hopkins
from pyclustertend import vat
from sklearn.preprocessing import scale

# load data
def load_data(file_name):
  res = None
  with open(file_name, 'rb') as f:
    res = chardet.detect(f.read())
  df =  pd.read_csv(file_name, encoding=res['encoding'])
  return df

"""## Data Cleaning, Exploration, and Feature Engineering

In this section, we will explore the dataset, reduce dimensionality, remove any erroneous data, and engineer new features in order to improve the accuracy of models.

### Loading Exploratory Data

Here, we will load our data and perform some basic cleaning to make it explorable.
"""

explore_df = load_data("spotify-2023.csv")

# Do some data cleaning, rationale for this will be explained in a future step
explore_df = explore_df.drop([574])
explore_df['streams'] = explore_df['streams'].astype(int)

"""### Basic Analysis of Dataset
Here, we will look at some of the basic properties of the dataset such as its shape, the distribution of `streams`, and looking at the top songs in the dataset.
<br/><br/>

Through doing so, we can see that the dataset is bottom-heavy and it's most sommon for songs to have fewer streams. The dataset also has 24 features which is a high number for dimensionality, so it needs to be reduced. Some basic observations of the top songs show that more recent songs tend to be popular. It also shows an example of a popular song that wasn't in the charts ("Dance Monkey" and "Closer" were never in the spotify charts)
"""

print("Shape of Data:", explore_df.shape)
display(explore_df["streams"].hist(bins=100, legend=True))
display(explore_df.sort_values("streams", ascending=False).head(10))

"""### Exploring Feature Correlation

Here, we will print out a correlation matrix for the data. We will first print out a matrix that only shows strong correlation, then one with medium correlation, and then we will print out a matrix that shows weak correlation. This will help us decide which features we can eliminate in the data cleaning stage.
<br/><br/>

Through doing this, we found that the features that have the highest correlation with `streams` was the amount of playlists it was in on various different platforms along with the release year.  
"""

# One hot encode keys for corr chart
one_hot = pd.get_dummies(explore_df['key'])
explore_df = explore_df.drop('key',axis = 1)
explore_df = explore_df.join(one_hot)

# Generate data cleaning
pd.set_option('display.max_columns', 30)
cm = abs(explore_df.corr())

# High correlation only
display("High Correlation")
display(cm[cm > .5])

# Medium correlation only
display("Medium Correlation")
display(cm[cm > .2][cm < .5])

# Low correlation only
display("Low Correlation")
display(cm[cm < .2])

"""### Detecting NaN Values

Finding NaN values is important as these missing values can affect the results of our models. We need to identify rows with NaNs so that we can handle them in the next step. Through doing this, we can see that ~10% of records in `key` are NaNs and ~5% in `in_shazam_charts` are NaNs.
"""

print("Number of NaNs:\n", explore_df.isna().sum())

"""### Handling Nans and Reducing Dimensionality

When exploring the data, the first thing we do is check for NaN values in the data. We found some in the `in_shazam_charts` row. Due to the relative unpopularity of Shazam compared to Spotify or Apple Music, we decided that dropping this feature would be the best path forward.
<br/><br/>

We also found some NaNs in the `key` feature. Though our correlation chart showed weak correlation between `key` and `streams`, domain specific knowledge suggests that some music keys are more popular than others (i.e. C and G major are most popular for pop songs), so we decided that keeping this feature would be good. We couldn't find a good set of other features that correlated with `key`, so doing imputation would have been challenging. As a result, we ended up assigning all missing values to the fake key 'H'. Later on, when doing feature engineering, we will address how we handle this value.
<br/><br/>



We also decided to drop `released_day`, `released_month` and `track_name`. `track_name` was dropped as there likely is not a significant correlation between track name and popularity, and because it would be challenging to properly numerically encode. `released_day` and `released_mont` was dropped as the specific month and day a song was released on has low correlation with the number of streams a song will get.
<br/><br/>

We also enoded `mode` into a numerical value to make it easier to work with and dropped row 574, as it was formatted improperly.
"""

def clean_df(df):
  # Columns we want to drop
  cols = ["in_shazam_charts", "released_day", "track_name", "released_month"]
  df = df.drop(columns=cols)

  # Check for correlations in key
  correlation = []
  for name, data in df.items():
      if name != "key":
          key = df["key"].astype('category').cat.codes
          res = df[name].astype('category').cat.codes
          correlation.append(key.corr(res))

  # There is no significant correlation, so no way to impute a specific value
  for corr in correlation:
    assert(corr < .2)

  # Use dummy key 'H' to replace nan
  s = set(i for i in df['key'])
  df["key"] = df["key"].fillna("H") # H does not exist in keys


  # Replace major/minor in mode with 0 and 1
  df["is_minor"] = df["mode"].astype('category').cat.codes
  df = df.drop(columns="mode")

  # this specific row has bugged data (there is a string instead of a number in the
  # streams column) which makes this row unusable for us
  df = df.drop([574])

  df['streams'] = df['streams'].astype(int)

  return df

"""### Encoding music keys

Encoding the musical key into a numerical value is a challenge. We could one-hot encode the values, but that would leave us with 12 new features (curse of dimensionality). One-hot encoding the values would also lead to an improper representation of distance between the keys. For instance, C and G are right next to each other on the [circle of fifths](https://en.wikipedia.org/wiki/Circle_of_fifths) but they would have the same distance between them as C and F# which are on opposite ends of the circle of fifths.
<br/><br/>

To encode this in an accurate way, we chose to create 2 features: `key_x` and `key_y`. These would be the X and Y coordinates of where the note would be on the circle of fifths. This creates a more accurate numberical representation of the notes and is very cool!
"""

keys = ["C", "G", "D", "A", "E", "B", "F#", "C#", "G#", "D#", "A#", "F"]
increment = (2 * math.pi) / len(keys)
fifths = {}
for i in range(len(keys)):
    fifths[keys[i]] = (math.sin(i * increment), math.cos(i * increment))
print(fifths)
points = fifths.values()
x = [x for x,y in points]
y = [y for x,y in points]

fig, ax = plt.subplots()
ax.scatter(x, y)
for i, txt in enumerate(fifths):
      ax.annotate(txt, (x[i] + 0.05, y[i]))

def circle_fifths_encode(df):
  x_col = []
  y_col = []
  for index, row in df.iterrows():
      # default to 0, 0 for our "H" key
      x = fifths.get(row["key"], (0, 0))[0]
      y = fifths.get(row["key"], (0, 0))[1]
      if x < 0.001:
          x = 0
      if y < 0.001:
          y = 0
      x_col.append(x)
      y_col.append(y)

  df = df.drop(columns=["key"])

  df.loc[:,"key_x"] = x_col
  df.loc[:,"key_y"] = y_col
  return df

"""### Coalescing metrics from different platforms

The dataset contains the charting and playlist frequency split between different platforms. Since these are all generally correlated, we can combine some of these features together.
<br/><br/>
We can combine the `in_[PLATFORM]_playlists` features by adding them together to get the total number of playlists the song is in across all platforms.

We can combine the `in_[PLATFORM]_charts` by taking the average chart position between the three services. If one is missing from a platform's chart (i.e. it didn't make the chart), it will be assumed to be at the last possible position on the chart for that platform.
"""

# coalesce playlists by doing a summation of them
playlist_cols = [ "in_apple_playlists",  "in_spotify_playlists", "in_deezer_playlists"]
chart_cols = [ "in_apple_charts", "in_spotify_charts", "in_deezer_charts"]

def coalesce_playlists(df):
  max_chart = {
    "in_apple_charts": df["in_apple_charts"].max(),
    "in_spotify_charts": df["in_spotify_charts"].max(),
    "in_deezer_charts": df["in_deezer_charts"].max()
  }

  playlist_col = []
  chart_col = []
  for index, row in df.iterrows():
    playlist_sum = 0
    chart_sum = 0
    for col in playlist_cols:
      playlist_sum += int(str(row[col]).replace(',', ''))
    for col in chart_cols:
      cur_rank = int(str(row[col]).replace(',', '')) / max_chart[col]
      chart_sum += max_chart[col] if cur_rank == 0 else cur_rank
    playlist_col.append(playlist_sum)
    chart_col.append(chart_sum / len(chart_cols))


  df = df.drop(columns=playlist_cols)
  df = df.drop(columns=chart_cols)


  df.loc[:,"total_playlists"] = playlist_col
  df.loc[:,"average_chart"] = chart_col

  df.to_csv("test.csv", sep='\t', encoding='utf-8')
  return df

"""### Encoding Artist's Name

The popularity of an artist definitely impacts the popularity of their songs - to encode the artist names with some measure of popularity we decided to use a square root curve. The idea is that if two artists of similar popularity work together on a song, the song is likely to reach similar levels of audience, but if a very popular creator collaborates with one that is not so popular, the relatively unpopular creator will likely get a boost by collaborating with the larger one. We chose average number of streams as our popularity metric.
"""

# Replace artist with popularity metrics
col = "artist(s)_name"

def artist_popularity_metric(df, pow):
  artist_pop = defaultdict(int)
  artist_num = defaultdict(int)
  for i, row in df.iterrows():
    name = row[col]
    n = name.split(", ")
    for i in n:
      artist_num[i] += 1
      artist_pop[i] += row["streams"]

  avgs = []
  for i, row in df.iterrows():
    names = row[col].split(", ")
    names_pop = [artist_pop[name] / artist_num[name] for name in names]
    mx = max(names_pop)
    for i in range(len(names_pop)):
      names_pop[i] = mx * math.pow((names_pop[i] / mx), pow)
    avgs.append(sum(names_pop) / len(names_pop))
  df.loc[:, col] = avgs
  df = df.rename(columns={col: 'mean_artist_appearance'})
  df = df.drop(columns=['artist_count'])
  return df

"""### Loading our Dataset

Now that we've defined how we will engineer and clean features, we will create a new DataFrame where we perform the feature engineering and cleaning to use when training and evaluating models.
<br/><br/>

When doing this, we create 3 featuresets for use when modeling to evaluate the best approach:

1. All features
2.  Only features about the song's musical characteristics
3.  Only features about the song and artist's popularity

"""

# Load in data again and call helper functions
df = load_data("spotify-2023.csv")
df = clean_df(df)
df = circle_fifths_encode(df)
df = coalesce_playlists(df)
df = artist_popularity_metric(df, 1/10)
df = df.sample(frac=1).reset_index(drop=True)

# Split data into x (features) and y (labels)

only_musical_features = ['bpm', 'danceability_%', 'valence_%',	'energy_%',	'acousticness_%',	'instrumentalness_%',	'liveness_%',	'speechiness_%',	'is_minor',	'key_x',	'key_y']
only_popularity_features = ['mean_artist_appearance', 'released_year', 'total_playlists', 'average_chart']


label_col = "streams"
x = df.loc[:, df.columns != label_col]
x_mf = x[only_musical_features]
x_pf = x[only_popularity_features]

# Print info about data
print("Shape of Data (All).               :", x.shape)
print("Shape of Data (Musical Features)   :", x_mf.shape)
print("Shape of Data (Popularity Features):", x_pf.shape)

x = np.array(x)
x_mf = np.array(x_mf)
x_pf = np.array(x_pf)

y = np.asarray(df.loc[:, label_col], dtype=int)

# Print more info
display("Head of Data (sorted by streams)")
display(df.sort_values("streams", ascending=False).head(10))
cm = abs(df.corr())
display("Correlation Matrix")
display(cm[cm > .0])

"""### Evaluating the clustering tendancy of the data

Later when evaluating models and deciding on hyperparameters, it will be useful to know how much the data clusters. Knowing this can help us evaluate if a clustering model such as DBSCAN or K-Means would be effective.

<br/><br/>
The Hopkins statistic is low for all of our datasets (<.25), suggesting that there are not clusters in the data.
"""

print("Hopkins Stat                      :", hopkins(scale(x), 200))
print("Hopkins Stat (Musical Features)   :", hopkins(scale(x_mf), 200))
print("Hopkins Stat (Popularity Features):", hopkins(scale(x_pf), 200))

"""## Modelling the Data
Now that we've explored the data and did feature engineering, we will begin using and evaluating different models to find the best one to use to predict popularity.

<br/><br/>

When doing this, we ran each model with 3 different featuresets to evaluate how well different approaches will work:

1. All features
2. Only features about the song's musical characteristics
3. Only features about the song and artist's popularity

### Linear Regression

We first decided to try using a simple regressor model: Linear Regression. It had a mean error that was about ~48% of the standard deviation of the data and got a CV score of ~63%
"""

def lin_reg(df, x_vals, y_vals):
  characteristic_cols = [i for i in df.columns if i != label_col]
  X_train, X_test, y_train, y_test = train_test_split(x_vals, y_vals, test_size=0.20)
  regression = LinearRegression()
  regression.fit(X_train, y_train)
  predicted = regression.predict(X_test)
  return y_test, predicted, regression

def calc_stats(df, y_test, predicted):
  mae = mean_absolute_error(y_test, predicted)
  print('Mean error:', mae)
  streams = [int(x) for x in df['streams'].tolist()]
  avg_streams = sum(streams) / len(streams)
  print('Average number of streams: ', str(avg_streams))
  print('Error as a percentage: ', str(mean_absolute_error(y_test, predicted) / avg_streams))
  res = statistics.pstdev(streams)
  print("Standard deviation of sample is : " + str(res))

def run_cross_val_score(df, regression, x_vals, y_vals, folds):
  scores = cross_val_score(regression, x_vals, y_vals, cv=folds)
  print("Average CV Score ("+str(folds)+" folds): ", (sum(scores)/len(scores)) * 100, "%")

print("All Features:")
y_test, pred, reg = lin_reg(df, x,y)
calc_stats(df, y_test, pred)
run_cross_val_score(df, reg, x, y, 5)
print("\n")

print("Musical Features:")
y_test, pred, reg = lin_reg(df, x_mf,y)
calc_stats(df, y_test, pred)
run_cross_val_score(df, reg, x_mf, y, 5)
print("\n")

print("Popularity Features:")
y_test, pred, reg = lin_reg(df, x_pf,y)
calc_stats(df, y_test, pred)
run_cross_val_score(df, reg, x_pf, y, 5)

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.neighbors import KNeighborsRegressor
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor
from sklearn.tree import DecisionTreeRegressor

"""### Polynomial Regression

We will now check for some nonlinear correlations between our features by running a 2-dimensional polynomial regression. We wanted to try this because of the skewed distribution of streams.

This improved our results somewhat, giving a mean absolute error about 36% of the standard deviation.
"""

def run_poly_reg(df, x, y, repeats):
  poly = PolynomialFeatures(degree=2, include_bias=False)
  poly_features = poly.fit_transform(x)
  std_dev = statistics.pstdev([int(x) for x in df['streams'].tolist()])
  total_mae = 0
  for i in range(repeats):
    y_test, y_pred, regression = lin_reg(df, poly_features, y)
    mae = mean_absolute_error(y_test, y_pred)
    total_mae += mae
  total_mae /= repeats
  print(f"MAE: {total_mae}")
  print(f"std dev of streams: {std_dev}")
  print(f"Error as a percentage: {total_mae / std_dev}")

print("All Features:")
run_poly_reg(df, x, y, 50)
print("\n")

print("Musical Features:")
run_poly_reg(df, x_mf, y, 50)
print("\n")

print("Popularity Features:")
run_poly_reg(df, x_pf, y, 50)

"""### K Nearest Neighbors


Next, we tried using K Nearest Neighbors with hyperparameter tuning. We hoped that by using KNN, we could get more accurate results as we didn't need our data to be linear for it to work best. We also hoped that by using GridSearch to tune hyperparameters that we would be able to further tune the performance of the model. As our clustering tendancy was not very high, we didn't hold out very high hopes that this would perform as well. It ultimately ended up performing similarly to our Linear Regressor.
"""

def run_knn(song_x, knn_range):
  scaler = StandardScaler()
  pca = PCA()
  regressor = KNeighborsRegressor(n_jobs=-1)
  pipeline = Pipeline([('scaler', scaler), ('pca', pca), ('knn', regressor)])
  r, c = song_x.shape
  param_grid = {
    'pca__n_components': list(range(5, c, 2)) + [None],
    'knn__n_neighbors': list(knn_range),
    'knn__weights': ['uniform', 'distance']
  }

  grid = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_absolute_error')
  grid.fit(song_x, y)
  print("Best Params  : " + str(grid.best_params_))
  print("Best Score   : " + str(grid.best_score_ * -1))

print("All Features:")
run_knn(x, range(10, 110, 5))
print("\n")

print("Musical Features:")
run_knn(x_mf, range(10, 110, 5))
print("\n")

print("Popularity Features:")
run_knn(x_pf, range(10, 110, 5))

"""### Decision Tree

Next, we decided to try a Decision Tree model as we should this could help with the skewed nature of the distribution of streams. The Decision Tree model performed much better than the previous models, with the MSE coming much closer to ~30% of the standard deviation of the dataset.

The best parameters were a max depth of around 8 and a minimum of over 12 samples per leaf. This makes some sense as the tree has to avoid overfitting to the training data.
"""

def run_dt(song_x):
  scaler = StandardScaler()
  pca = PCA()
  regressor = DecisionTreeRegressor()
  pipeline = Pipeline([('scaler', scaler), ('dtr', regressor)])

  param_grid = {
      'dtr__max_depth': list(range(1, 21)) + [None],
      'dtr__min_samples_leaf': list(range(10, 30))
  }
  grid = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_absolute_error')
  grid.fit(song_x, y)

  print("Best Params  : " + str(grid.best_params_))
  print("Best Score   : " + str(grid.best_score_ * -1))

print("All Features:")
run_dt(x)
print("\n")

print("Musical Features:")
run_dt(x_mf)
print("\n")

print("Popularity Features:")
run_dt(x_pf)

"""### Gradient Boosting

Next we tried a handful of ensemble decision tree regressors in an attempt to get a more accurate output. Simply averaging the results of 12-16 points (as a single decision tree would) might not always give the best results.


GradientBoostingRegressor is an ensemble classifier that trains decision trees in the ensemble sequentially, with each trying to correct the error of the trees before it.

This model performed about as well as any, with a MAE of about 28% of the standard deviation of the dataset.
"""

def run_gb(song_x):
  scaler = StandardScaler()
  regressor = GradientBoostingRegressor(max_features=12, max_depth=5, min_samples_leaf=12)
  #regressor = GradientBoostingRegressor(min_samples_leaf=12)
  #pipeline = Pipeline([('scaler', scaler), ('pca', pca), ('gbr', regressor)])
  pipeline = Pipeline([('scaler', scaler), ('gbr', regressor)])

  param_grid = {
      #'pca__n_components': list(range(7, 16)),
      #'gbr__max_features': list(range(5, 17)) + [None], # got 12
      'gbr__max_depth': list(range(3, 6)) + [None], #got 4
      #'gbr__min_samples_leaf': list(range(10, 20)), #Got 12
      #'gbr__n_estimators': list(range(100, 310, 50)), #got 300? but insignificant
      #'gbr__learning_rate': [0.05, 0.10], #got a lower one but insignificant
      #'gbr__subsample': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1] #got .9
  }
  grid = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_absolute_error')
  grid.fit(song_x, y)

  print("Best Params  : " + str(grid.best_params_))
  print("Best Score   : " + str(grid.best_score_ * -1))

print("All Features:")
run_gb(x)
print("\n")

print("Musical Features:")
run_gb(x_mf)
print("\n")

print("Popularity Features:")
run_gb(x_pf)

"""### Bagging and Adaboost Regressor

These models had comparable performance to the Gradient Boosting Regressor.

BaggingRegressor makes an ensemble of multiple decision trees from parts of the dataset.

Adaboost fits multiple copies of the decision tree on the dataset but with weights based on the error of each base classifier. This is intended for additional regressors to focus on more difficult cases.
"""

def run_bag_and_ada(song_x):
  scaler = StandardScaler()
  regressor = BaggingRegressor(estimator=DecisionTreeRegressor())
  pipeline = Pipeline([('scaler', scaler), ('br', regressor)])
  param_grid = {
      'br__n_estimators': list(range(20, 45, 5)),
      'br__max_samples': [0.05, 0.10, 0.15, 0.2],
  }
  grid = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_absolute_error')
  grid.fit(song_x, y)
  print("Best Params  : " + str(grid.best_params_))
  print("Best Score   : " + str(grid.best_score_ * -1))

  scaler = StandardScaler()
  regressor = AdaBoostRegressor(estimator=DecisionTreeRegressor())
  pipeline = Pipeline([('scaler', scaler), ('abr', regressor)])
  param_grid = {
      'abr__n_estimators': list(range(40, 110, 10)),
  }
  grid = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_absolute_error')
  grid.fit(song_x, y)
  print("Best Params  : " + str(grid.best_params_))
  print("Best Score   : " + str(grid.best_score_ * -1))



print("All Features:")
run_bag_and_ada(x)
print("\n")

print("Musical Features:")
run_bag_and_ada(x_mf)
print("\n")

print("Popularity Features:")
run_bag_and_ada(x_pf)

"""### Neural Net (Multi-Layer Perceptron)

MultiLayerPerceptron took a very long time to run, and was worse in accuracy than the decision tree-based ensemble classifiers. have less overfitting for new records There was consistently a tradeoff involving batch size; smaller batch sizes had better results but predictably had significantly worse training times, as the weights of the neural network had to be updated after checking just a few records

"""

from sklearn.neural_network import MLPRegressor

def run_nn(song_x):
  scaler = StandardScaler()
  pca = PCA()
  regressor = MLPRegressor()
  pipeline = Pipeline([('scaler', scaler), ('mlp', regressor)])


  param_grid = {
      'mlp__hidden_layer_sizes': [[16, 16, 16], [50, 50, 50]],
      'mlp__batch_size': [5, 20, 50]
  }
  grid = GridSearchCV(pipeline, param_grid, cv=2, scoring='neg_mean_absolute_error')
  grid.fit(song_x, y)

print("All Features:")
run_nn(x)
print("\n")

print("Musical Features:")
run_nn(x_mf)
print("\n")

print("Popularity Features:")
run_nn(x_pf)

"""## Outcome & Results

In our data exploration, we found that there wasn't much of a clustering tendency in the dataset. This led us to conclude that most songs are fairly unique, resulting in a somewhat sparse dataset. The sparseness of the data contributed to KNN and Linear Regression performing poorly as these are not optimal conditions for these models.

We also found that certain features had very little correlation with each other, and thus were inappropriate for PCA. The only model where PCA caused an accuracy increase was K nearest neighbors; for all other models it significantly reduced the accuracy and thus we decided not to do it.

One issue with the dataset is that some features have a very low correlation with the number of streams, while others were almost proxies for the number of streams. Features about the song itself such as 'Danceability' and the key it was written in added a lot of dimensionality while not being useful for prediction. What works best at predicting streams is the popularity of the artists and proxy features for song popularity such as being on charts and being in playlists. This aligns with historical trends in music where artists usually gain listeners after becoming popular, and these listeners continue listening even when the artist's musical style evolves. The importance of artist popularity can also be seen whenever a popular band does a cover of a less-popular band's song and receives more streams than the original less-popular band.

To prove this, we tried running our models with different subsets of the features. Running them without the popularity metrics and only the song characteristics led to significant reductions in accuracy relative to when all features were present. Further some models worked about as well with only the popularity metrics (the various decision tree ensemble classifiers + polynomial regression) as they did with all features present.

The best models were generally decision tree ensemble classifiers. The best one was GradientBoostingRegressor, whose mean absolute error was about 28% of the standard deviation of streams when all features were present. One reason why decision tree regressors might work better than linear regression is the skewed distribution of the y/streams values. The right combination of circumstances can lead to songs being breakout hits from the rest of the data set.

"""